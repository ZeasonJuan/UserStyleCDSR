import asyncio, json, time
from pathlib import Path
from openai import AsyncOpenAI
from transformers import AutoTokenizer

BASE_URL = "http://127.0.0.1:8010/v1" #markğŸ˜
API_KEY  = "dummy"
MODEL_ID = "phi3.5-mini"

CONCURRENCY = 64             # å¹¶å‘åº¦ï¼ˆæŒ‰æ˜¾å­˜+æ¨¡å‹å¤§å°è°ƒï¼‰
MAX_INPUT_LINES = 32         # æ¯æ¡åºåˆ—æœ€å¤šé€å¤šå°‘ item è¡Œï¼Œé˜²æ­¢promptè¿‡é•¿
OUTPUT_JSONL = "llm_summary_500.jsonl"


PROMPT_TEMPLATE = """You are an expert in recommendation systems. 
Given the texts of items a user interacted with, summarize the user's overall preference style.

INPUT (the text below is the concatenation of texts of items the user interacted with, most recent last):
{lines}

INSTRUCTIONS:
1) Focus on stable long-term interests, but also note recent short-term shifts if any.
2) Avoid copying item titles verbatim; generalize to concepts/attributes.
3) Prefer signals that repeat across multiple items; discount one-offs and popularity bias.
4) Keep it concise and structured.

OUTPUT:
A single paragraph summarizing the user's preference style, just provide the summary; no other wording is needed..
"""

client = AsyncOpenAI(base_url=BASE_URL, api_key=API_KEY)
HF_MODEL_ID = "microsoft/Phi-3.5-mini-instruct"  # ç”¨äºæœ¬åœ°è®¡æ•°çš„ HF æ¨¡å‹å
MODEL_CTX_LEN = 4096                              # vLLM å¯åŠ¨æ—¶çš„ --max-model-len
MAX_NEW_TOKENS = 256                              # ä½ åœ¨è¯·æ±‚é‡Œè®¾çš„ max_tokens
SAFETY_MARGIN = 32  


PROMPT_BUDGET = MODEL_CTX_LEN - MAX_NEW_TOKENS - SAFETY_MARGIN

_tokenizer = AutoTokenizer.from_pretrained(
    HF_MODEL_ID,
    use_fast=True,
    local_files_only=True  # æ²¡ç½‘ç¯å¢ƒè¯»æœ¬åœ°ç¼“å­˜
)

def _count_chat_tokens(messages):
    """
    è®¡ç®— chat æ¶ˆæ¯åœ¨å½“å‰æ¨¡å‹ä¸‹çš„ token æ•°ã€‚
    ä¼˜å…ˆä½¿ç”¨ chat_templateï¼ˆä¸ vLLM å¯¹é½ï¼‰ï¼Œè‹¥ä¸å¯ç”¨åˆ™é€€åŒ–åˆ°æ‹¼æ¥æ–‡æœ¬ã€‚
    """
    try:
        ids = _tokenizer.apply_chat_template(
            messages,
            tokenize=True,
            add_generation_prompt=True,  # ä¸ chat.completions ä¸€è‡´
        )
        return len(ids)
    except Exception:
        # é€€åŒ–ï¼šç®€å•æ‹¼æ¥è§’è‰²ä¸å†…å®¹ï¼ˆä¸å®Œå…¨ç²¾ç¡®ï¼Œä½†å¯ä½œä¸ºå…œåº•ï¼‰
        concat = []
        for m in messages:
            role = m.get("role", "user")
            content = m.get("content", "")
            concat.append(f"<{role}>\n{content}\n</{role}>\n")
        toks = _tokenizer("".join(concat), add_special_tokens=False).input_ids
        return len(toks)


def _truncate_user_to_budget(messages, budget_tokens):
    """
    è‹¥è¶…å‡º budget_tokensï¼Œåˆ™åªæˆªæ–­æœ€åä¸€æ¡ user æ¶ˆæ¯çš„ contentã€‚
    ç”¨äºŒåˆ†æ³•æŒ‰å­—ç¬¦é•¿åº¦æˆªæ–­ï¼Œå¹¶ä»¥ç©ºç™½å¤„åˆ†è¯è¾¹ç•Œä¸ºä¼˜å…ˆã€‚
    è¿”å›: (new_messages, info_dict)
    """
    if not messages:
        return messages, {"truncated": False, "tokens": 0}

    total_tokens = _count_chat_tokens(messages)
    if total_tokens <= budget_tokens:
        return messages, {"truncated": False, "tokens": total_tokens}

    # æ‰¾åˆ°æœ€åä¸€ä¸ª user æ¶ˆæ¯ï¼ˆé€šå¸¸å°±æ˜¯ä½ æ‹¼ prompt çš„é‚£æ¡ï¼‰
    idx = None
    for i in range(len(messages) - 1, -1, -1):
        if messages[i].get("role") == "user":
            idx = i
            break
    if idx is None:
        # å¦‚æœæ²¡æœ‰ userï¼Œå°±ä¸åŠ¨
        return messages, {"truncated": False, "tokens": total_tokens}

    user_content = messages[idx].get("content", "")
    if not user_content:
        return messages, {"truncated": False, "tokens": total_tokens}

    # äºŒåˆ†æœç´¢å¯å®¹çº³çš„æœ€å¤§å­—ç¬¦å‰ç¼€
    lo, hi = 1, len(user_content)
    best_len = 0

    def tokens_with_prefix(nchars: int) -> int:
        new_user_text = user_content[:nchars]
        # ä¼˜å…ˆåˆ‡åˆ°ä¸Šä¸€ä¸ªç©ºç™½ï¼Œé¿å…æˆªæ–­åœ¨è¯ä¸­éƒ¨
        ws = new_user_text.rfind(" ")
        if 64 < ws < nchars:  # ä¿è¯ä¸è¦è¿‡æ—©åˆ‡å¤ªå¤š
            new_user_text = new_user_text[:ws]
        tmp = list(messages)
        tmp[idx] = {**tmp[idx], "content": new_user_text}
        return _count_chat_tokens(tmp)

    # å¦‚æœè¿ 1 ä¸ªå­—ç¬¦éƒ½æ”¾ä¸ä¸‹ï¼Œå°±ç›´æ¥è¿”å›æœ€çŸ­å†…å®¹
    if tokens_with_prefix(1) > budget_tokens:
        new_msgs = list(messages)
        new_msgs[idx] = {**new_msgs[idx], "content": ""}
        return new_msgs, {"truncated": True, "tokens": _count_chat_tokens(new_msgs)}

    # äºŒåˆ†æ‰¾æœ€å¤§å¯è¡Œå‰ç¼€
    while lo <= hi:
        mid = (lo + hi) // 2
        cnt = tokens_with_prefix(mid)
        if cnt <= budget_tokens:
            best_len = mid
            lo = mid + 1
        else:
            hi = mid - 1

    # åº”ç”¨æˆªæ–­
    new_text = user_content[:best_len]
    ws = new_text.rfind(" ")
    if 64 < ws < best_len:
        new_text = new_text[:ws]

    new_messages = list(messages)
    new_messages[idx] = {**new_messages[idx], "content": new_text}
    final_tokens = _count_chat_tokens(new_messages)
    return new_messages, {"truncated": True, "tokens": final_tokens}

async def one_call(seq_items, idx, sem):
    lines = "\n".join(seq_items[:MAX_INPUT_LINES])
    prompt = PROMPT_TEMPLATE.format(lines=lines)

    # åŸæ¥çš„ messages
    messages = [
        {"role": "system", "content": "Return only the summary paragraph."},
        {"role": "user", "content": prompt},
    ]

    # è®¡ç®—å¹¶æˆªæ–­åˆ°é¢„ç®—ï¼ˆPROMPT_BUDGETï¼‰
    messages, trunc_info = _truncate_user_to_budget(messages, PROMPT_BUDGET)

    async with sem:
        t0 = time.perf_counter()
        resp = await client.chat.completions.create(
            model=MODEL_ID,
            messages=messages,
            temperature=0.0,
            seed=42,
            max_tokens=MAX_NEW_TOKENS,
        )
        t1 = time.perf_counter()

    content = resp.choices[0].message.content
    usage = getattr(resp, "usage", None)
    pt = getattr(usage, "prompt_tokens", 0) if usage else 0
    ct = getattr(usage, "completion_tokens", 0) if usage else 0

    return {
        "index": idx,
        "latency_sec": round(t1 - t0, 4),
        "prompt_tokens": pt or trunc_info.get("tokens", 0),  # vLLMä¼šè¿”å›æ›´å‡†çš„usage
        "completion_tokens": ct,
        "truncated": trunc_info.get("truncated", False),
        "output": content,
    }


INPUT_SEQ2TEXT_TSV = "../dataset/office-arts/OA/Office.seq2text.tsv"       # ä½ çš„ (item_newid_seq -> items_text) è¾“å…¥æ–‡ä»¶
OUTPUT_SEQ2LLM_TSV = "../dataset/office-arts/OA/Office.seq2summary.tsv"    # äº§å‡ºçš„ (item_newid_seq -> llm_summary) æ–‡ä»¶
JOINER = " "                                      # ä½ å½“å‰çš„ joiner

# ====== å°å·¥å…·ï¼šè¯»å– seq2text TSV ======
def load_seq2text_tsv(path):
    """
    è¯»å–ä¸¤åˆ— TSVï¼š<seq_str>\t<items_text>
    è¿”å›åˆ—è¡¨ [(seq_str, items_text), ...]
    """
    out = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            line = line.rstrip("\n")
            if not line:
                continue
            parts = line.split("\t", 1)
            if len(parts) != 2:
                continue
            seq_str, items_text = parts
            out.append((seq_str, items_text))
    return out

async def main():
    # è¯»å– 500 æ¡
    pairs = load_seq2text_tsv(INPUT_SEQ2TEXT_TSV)  # [(seq_str, items_text), ...]
    n = len(pairs)
    print(f"Loaded {n} sequences from {INPUT_SEQ2TEXT_TSV}")

    sem = asyncio.Semaphore(CONCURRENCY)
    t0 = time.perf_counter()
    tasks = []

    for i, (seq_str, items_text) in enumerate(pairs):
        # ä½ çš„ joiner ç°åœ¨æ˜¯ " "ï¼Œæ— æ³•æŒ‰ item è¿˜åŸï¼Œåªèƒ½æ•´æ®µä½œä¸ºä¸€è¡Œé€å…¥
        seq_items = [items_text]
        # å¦‚æœä»¥åæŠŠ joiner æ¢æˆ " ||| "ï¼Œå¯ä»¥æ”¹æˆï¼š
        # seq_items = [s for s in items_text.split(" ||| ") if s.strip()]
        tasks.append(asyncio.create_task(one_call(seq_items, i, sem)))

    results = await asyncio.gather(*tasks)
    t1 = time.perf_counter()

    # å†™å‡º
    with open(OUTPUT_SEQ2LLM_TSV, "w", encoding="utf-8") as fout:
        for r in results:
            idx = r["index"]
            seq_str = pairs[idx][0]
            summary = (r["output"] or "").strip().replace("\n", " ")
            fout.write(f"{seq_str}\t{summary}\n")

    # ç»Ÿè®¡
    total_pt = sum(r.get("prompt_tokens", 0) for r in results)
    total_ct = sum(r.get("completion_tokens", 0) for r in results)
    total = len(results)
    dur = t1 - t0
    print(f"Wrote {total} lines to {OUTPUT_SEQ2LLM_TSV}")
    print(f"Done {total} requests in {dur:.2f}s | {total/dur:.2f} req/s")
    print(f"Tokens prompt/completion: {total_pt}/{total_ct} | total {total_pt+total_ct}")

if __name__ == "__main__":
    asyncio.run(main())